{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d722956-9ace-4547-8f8c-cdba22cd859f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# New and improved Parflow IO routines, integration with `xarray`, and some nifty tools for you non-Parflowheads\n",
    "\n",
    "This notebook details some of the new Parflow IO routines in the Parflow python package. As of Jan 27, 2022 they are not quite in the main repo, so can't be `pip` installed yet. But that should happen soon - for those interested here's the [pull request](https://github.com/parflow/parflow/pull/365). This tutorial can also be run via binder through [this link](https://mybinder.org/v2/gh/arbennett/group_meeting_feb1/HEAD), alternatively here is [the GitHub repo](https://github.com/arbennett/group_meeting_feb1).\n",
    "\n",
    "## First up - why?\n",
    "\n",
    "I first undertook this rewrite because I found some flaws in the existing workflow for the Parflow IO routines. First, there was an underlying memory leak because of the way that the ParflowIO package holds a copy of data in the background and you interact with the data by copying it (or pieces of it). Second, it was very slow to pull out timeseries because of the way that ParflowIO reads the file header and subgrid headers every time you open a file, which ends up being a rather large overhead when you are reading thousands of files. This new method addresses both of these problems, as well as offers some other nice benefits. It is written in pure python so it is easier to understand and modify. I have also written a backend to the [xarray](https://xarray.pydata.org/en/stable/) python package, which is massively useful for data analysis.\n",
    "\n",
    "## What you will get out of this tutorial/notebook/demo/thing\n",
    "\n",
    "Okay, so that was the motivation for me to do all of this work, but what's in it for you? By putting together this notebook I hope to convince you that the new tools offer a compelling and performant way to analyze Parflow output. For those not really into Parflow specifically, I will show you some cool things like:\n",
    "\n",
    " * Loading data over the network from Google drive via [gdrivefs](https://github.com/fsspec/gdrivefs) (And more generally [fsspec](https://filesystem-spec.readthedocs.io/en/latest/?badge=latest#))\n",
    " * How xarray and the larger [Pangeo ecosystem](https://pangeo.io/) can make your data-analysis-life much easier and intuitive\n",
    " \n",
    "Let's get to it - as always, starting with some imports and fixing up the plotting style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce6aa80-f948-4455-a39f-466646370a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import parflow as pf\n",
    "import gdrivefs\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Set a bigger default plot size\n",
    "mpl.rcParams['figure.figsize'] = (12, 8)\n",
    "mpl.rcParams['font.size'] = 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77e8f2c-3b19-4293-b12b-9c4f0774cf7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Intro to `gdrivefs`, or how to interact with Google drive via the notebook\n",
    "\n",
    "Because `git` doesn't like to have large datasets put into the repo I had to get a bit creative in making this tutorial open and accessible. There's this awesome library [fsspec](https://filesystem-spec.readthedocs.io/en/latest/?badge=latest#) that I mentioned earlier, which basically lets you access different filesystems directly in python. I've got a whole [(unfinished) blog post](http://arbennett.github.io/software,/machine-learning/2021/08/18/ml_infrastructure_for_eess_beginners.html) on using fsspec - but wanted to use Google Drive here because we have unlimited storage at UA. Enter `gdrivefs`, which is basically an implementation of `fsspec` on top of Google Drive. It's pretty bare-bones but works pretty well in my experience.\n",
    "\n",
    "To get started, you can run the cell below which points to my files for the rest of the tutorial. When starting up the `gdrivefs` session you'll have to enter a token and allow access from the link provided. Once you have granted access you'll get a code that you should copy and paste into the box that appears below the cell. With that you'll have access to the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe62e9ac-199e-4c78-a0b1-e985c30e4010",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = 'browser'\n",
    "data_id = '1DMXwWqrF3-vhdYZcUU-RAYFYQv6BuL-T'\n",
    "gdfs = gdrivefs.GoogleDriveFileSystem(token=token, root_file_id=data_id)\n",
    "gdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac95190d-84be-4c26-b06d-5bc5659f1db2",
   "metadata": {},
   "source": [
    "### Looking at the data\n",
    "\n",
    "With the connection to Google drive open you can interact with some Unix-like interfaces. First, let's just poke around and see the files using `ls` which just lists files and directories. You will see there are two directories here, `conus1_data` and `taylor_data`. Let's also see what's in the `conus1_data` directory by using `ls(\"conus1_data\")` - you should see a bunch of files that we will analyze here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3011c60-8932-4ff6-af36-f73870e7c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdfs.ls(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47b33d1-932f-45f2-be43-c94761e08700",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdfs.ls(\"conus1_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa82d94e-3ef2-49c6-b275-2c3969486393",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf2cc79-c444-4bee-9c64-cd480d6f5f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'CONUS.2004.out.press.00000.pfb'\n",
    "gdfs.download(f'/conus1_data/{filename}', \n",
    "              f'{filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d64082-4ff4-4d96-98ae-46cef0bbc95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pressure = pf.read_pfb(filename)\n",
    "print(pressure.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19084c3-2717-4907-b594-a0631da8c052",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.mean(pressure, axis=0))\n",
    "plt.colorbar(shrink=0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "group_meeting",
   "language": "python",
   "name": "group_meeting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
